# -*- coding: utf-8 -*-
"""Training code Deep Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w1zELVOvGNwrmsJWi003O6g4n3V-b9Kj
"""

!wget https://fisd-dataset.s3.amazonaws.com/fisd-asanti-twi-90p.zip

!unzip fisd-asanti-twi-90p.zip

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install datasets

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import os
from datasets import Dataset

!pip install pandas datasets transformers

import os
import pandas as pd
import re
import unicodedata
from datasets import Dataset, DatasetDict

#storing paths for csv and audio is variables
train_csv_path = "/content/fisd-asanti-twi-90p/data.csv"
audio_base_path = "/content/fisd-asanti-twi-90p/audios/"

##########################################
# Define cleaning function for training
##########################################
def clean_text_train(text, allowed_symbols=None):
    """
    Cleans the input text by:
    - Converting to uppercase.
    - Normalizing Unicode characters.
    - Removing non-alphanumeric characters except those specified in allowed_symbols.
    - Stripping extra whitespace.

    Args:
        text (str): The text to clean.
        allowed_symbols (set): A set of symbols to retain in the text.

    Returns:
        str: The cleaned text.
    """
    # Convert to uppercase
    #text = text.upper()

    # Normalize Unicode to NFC
    text = unicodedata.normalize("NFC", text)

    # Define default allowed characters if none provided
    if allowed_symbols is None:
        allowed_symbols = set("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ƆƐabcdefghijklmnopqrstuvwxyzɛɔ ")

    # Create a set of allowed characters (letters, digits, space, and allowed symbols)
    allowed_chars = set("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ƆƐabcdefghijklmnopqrstuvwxyzɛɔ").union(allowed_symbols)

    # Remove characters not in allowed_chars
    cleaned = "".join(ch for ch in text if ch in allowed_chars)

    # Remove extra whitespace
    cleaned = re.sub(r"\s+", " ", cleaned).strip()

    return cleaned


# Load training CSV file

train_df = pd.read_csv(train_csv_path, delimiter="\t", on_bad_lines='skip')

# Remove 'Unnamed: 0' column if it exists
train_df = train_df.drop(['Unnamed: 0'], axis=1, errors='ignore')

# Print a few rows before cleaning for verification
print("=== Sample Rows Before Cleaning ===")
print(train_df.head(5))


# Update 'Audio Filepath' by removing the lacounda
train_df['Audio Filepath'] = train_df['Audio Filepath'].apply(
    lambda x: os.path.join(audio_base_path, os.path.basename(x))
)


# Clean the 'Transcription' column
# Specify allowed symbols (add any symbols you want to retain)
# Example: periods, commas, percent, ampersand, hyphen, Ghanaian Cedi symbol
allowed_symbols_train = set(".,")

train_df['Transcription'] = train_df['Transcription'].apply(
    lambda x: clean_text_train(x, allowed_symbols=allowed_symbols_train) if isinstance(x, str) else x
)

# Printing a few rows after cleaning to verify the cleaning step
print("\n=== Sample Rows After Cleaning ===")
print(train_df.head(5))


# Shuffling and splitting the dataset

shuffled_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)
split_ratio = 0.8  # 80% for training
split_index = int(len(shuffled_df) * split_ratio)

train_df = shuffled_df[:split_index]       # Training data
validation_df = shuffled_df[split_index:]  # Validation data


# Converting to match Hugging Face Datasets

train_dataset = Dataset.from_pandas(train_df)
validation_dataset = Dataset.from_pandas(validation_df)

common_voice = DatasetDict({
    "train": train_dataset,
    "validation": validation_dataset
})

print(common_voice)


# Printing a few samples from the train dataset for final verification

print("\n=== Sample Training Examples ===")
for i in range(min(3, len(train_dataset))):
    print(f"Example {i+1}")
    print("Audio Filepath:", train_dataset[i]["Audio Filepath"])
    print("Transcription:", train_dataset[i]["Transcription"])
    print()


# Print a few samples from the validation dataset for final verification

if len(validation_dataset) > 0:
    print("\n=== Sample Validation Examples ===")
    for i in range(min(3, len(validation_dataset))):
        print(f"Example {i+1}")
        print("Audio Filepath:", validation_dataset[i]["Audio Filepath"])
        print("Transcription:", validation_dataset[i]["Transcription"])
        print()
else:
    print("\nNo validation examples found.")

#looking at the features after clean up
common_voice["train"].features

from datasets import Audio

# If you don't need the Translation column, remove it
common_voice = common_voice.remove_columns(["Translation"])

# Rename columns to match the source format
common_voice = common_voice.rename_column("Audio Filepath", "audio")
common_voice = common_voice.rename_column("Transcription", "sentence")

# Casting the audio column to an Audio feature.
# Specifying the sampling rate of 16kHz
common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16000))

# Printing the features to confirm
print(common_voice["train"].features)

import os
from datasets import Audio
#clearing bad files that will cause errors:

# Names of the bad files
bad_files = [
    "AsantiTwiFm20-A SLRKMb-Tmp012-ql0LnX.ogg",
    "AsantiTwiFm20-A SLRKMb-Tmp089-eRlzWZ.ogg",
    "AsantiTwiFm20-A SLRKMb-Tmp011-bcTYyx.ogg",
    "AsantiTwiFm18-MHdyb4ij-Tmp001-6afLtE.ogg",
    "AsantiTwiMa20-SQF23O2h-Tmp014-T6agOK.ogg",

]

# Disabling audio decoding so that filtering won't try to load the audio
common_voice = common_voice.cast_column("audio", Audio(decode=False))

# Filtering out the rows with the bad files
common_voice = common_voice.filter(
    lambda x: os.path.basename(x["audio"]["path"]) not in bad_files
)

# Re-enabling audio decoding after  removing bad files
common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16000, decode=True))

# Printing to verify the dataset no longer contains those files
print(common_voice)

from transformers import WhisperProcessor

# Do not specify 'language' or set it to None
processor = WhisperProcessor.from_pretrained("openai/whisper-small", task="transcribe")

def prepare_dataset(example):
    audio = example["audio"]
    example = processor(
        audio=audio["array"],
        sampling_rate=audio["sampling_rate"],
        text=example["sentence"],
    )
    example["input_length"] = len(audio["array"]) / audio["sampling_rate"]
    return example

common_voice = common_voice.map(
    prepare_dataset,
    remove_columns=common_voice["train"].column_names,
    num_proc=1
)

print(common_voice)

import torch
from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any  # This should be an instance of WhisperProcessor

    def __call__(
        self,
        features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        """
        This method takes a batch (list) of examples as input, each example being a dictionary
        containing at least 'input_features' and 'labels'. It returns a dictionary of padded
        tensors ready for model input.
        """

        # Extract the input features (audio features). Each example has 'input_features' as a list with one element.
        input_features = [{"input_features": f["input_features"][0]} for f in features]

        # Pad the input features to a common length using the processor's feature extractor.
        # This creates a batch with 'input_features' as a padded tensor (mel spectrogram for Whisper).
        batch = self.processor.feature_extractor.pad(
            input_features,
            return_tensors="pt"
        )

        # Extract labels. Each example has a 'labels' field, which is a list of token IDs.
        label_features = [{"input_ids": f["labels"]} for f in features]

        # Pad the labels using the processor’s tokenizer. This returns a dictionary with
        # 'input_ids' and 'attention_mask' for the labels.
        labels_batch = self.processor.tokenizer.pad(
            label_features,
            return_tensors="pt"
        )

        # Replace padding token IDs with -100 so that they are ignored by the loss function.
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        # If all sequences start with a BOS (beginning-of-sequence) token, remove it.
        # This is sometimes done to align with how the model expects input.
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        # Add the processed labels to the batch.
        batch["labels"] = labels

        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

!pip install jiwer
!pip install evaluate

import evaluate

# Load the Word Error Rate (WER) metric
metric = evaluate.load("wer")

# If you have a custom normalizer for Twi, define it here.
import numpy as np
import os
import re
import string
import unicodedata
import torch
import evaluate
from dataclasses import dataclass
from typing import Any, Dict, List, Union
from transformers import WhisperProcessor
from datasets import Audio, Dataset, DatasetDict


# Initialising Processor and Metric
# Assuming we have chosen a multilingual model capable of Twi and English code switching to train it on our twi dataset.
processor = WhisperProcessor.from_pretrained(
    "openai/whisper-small",
    task="transcribe"  # no language specified, letting Whisper detect language
)

metric = evaluate.load("wer")

#Defining the Twi Financial Normalizer

def twi_financial_normalizer(text: str) -> str:
    text = text.lower()
    text = unicodedata.normalize("NFC", text)

    # Allowed characters include letters, digits, financial symbols, punctuation used in financial contexts
    allowed_chars = set("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ƆƐ")
    text = "".join(ch for ch in text if ch in allowed_chars)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# Step 4: Define the Compute Metrics Function

# def compute_metrics(pred):
#     pred_ids = pred.predictions
#     label_ids = pred.label_ids

#     # Replace -100 with pad_token_id
#     label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

#     # Decode predictions and references
#     pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
#     label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

#     # Compute orthographic WER (no normalization)
#     wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)

#     # Normalize predictions and references
#     pred_str_norm = [twi_financial_normalizer(p) for p in pred_str]
#     label_str_norm = [twi_financial_normalizer(l) for l in label_str]

#     # Filter out empty references to avoid errors in WER computation
#     pred_str_norm = [
#         pred_str_norm[i] for i in range(len(pred_str_norm))
#         if len(label_str_norm[i]) > 0
#     ]
#     label_str_norm = [
#         label_str_norm[i] for i in range(len(label_str_norm))
#         if len(label_str_norm[i]) > 0
#     ]

#     # Compute WER on normalized text
#     wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)

#     return {"wer_ortho": wer_ortho, "wer": wer}


def compute_metrics(pred):
    # Extracting predictions and labels
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # Replacing -100 with the pad token id so that we decode properly
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    # Decoding predictions and references
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

    # Normalizing predictions and references
    pred_str_norm = [twi_financial_normalizer(p) for p in pred_str]
    label_str_norm = [twi_financial_normalizer(l) for l in label_str]

    # Filtering out empty references to avoid errors in WER computation
    pred_str_norm = [
        pred_str_norm[i] for i in range(len(pred_str_norm))
        if len(label_str_norm[i]) > 0
    ]
    label_str_norm = [
        label_str_norm[i] for i in range(len(label_str_norm))
        if len(label_str_norm[i]) > 0
    ]



    # Computing orthographic WER (no normalization)
    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)

    cer = 100 * compute_character_error_rate(pred_str, label_str)

    # Computing WER on normalized text
    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)

    return {"wer_ortho": wer_ortho, "wer": wer, "cer": cer}

    # Computing CER (Character Error Rate)
    # We define a helper function to compute CER for a list of predictions and references
def compute_character_error_rate(predictions, references):
    """
    Compute the Character Error Rate (CER) between a list of predictions and references.
    CER is defined as the edit distance between the predicted and reference text,
    divided by the number of characters in the reference, multiplied by 100.
    """
    total_distance = 0
    total_chars = 0
    for pred, ref in zip(predictions, references):
        # Remove extra spaces if needed and measure distance
        pred = pred.strip()
        ref = ref.strip()

        # Compute edit distance for characters
        dist = levenshtein_distance(ref, pred)
        total_distance += dist
        total_chars += len(ref)

    # Avoid division by zero if some references are empty
    if total_chars == 0:
        return 0.0

    return total_distance / total_chars


def levenshtein_distance(s1, s2):
    """
    Compute the Levenshtein distance (edit distance) between two strings at the character level.
    This is a standard dynamic programming approach:
    - distance[i][j] represents the edit distance between s1[:i] and s2[:j].
    """
    m = len(s1)
    n = len(s2)

    # Initialize a (m+1)x(n+1) matrix
    distance = np.zeros((m+1, n+1), dtype=int)

    # Base cases: distance from empty string
    for i in range(m+1):
        distance[i][0] = i
    for j in range(n+1):
        distance[0][j] = j

    # Compute distances
    for i in range(1, m+1):
        for j in range(1, n+1):
            cost = 0 if s1[i-1] == s2[j-1] else 1
            distance[i][j] = min(
                distance[i-1][j] + 1,     # deletion
                distance[i][j-1] + 1,     # insertion
                distance[i-1][j-1] + cost # substitution
            )

    return distance[m][n]
    return {"wer": wer, "cer": cer}

#This is the final code for compute_metrics:
def compute_metrics(pred):
    # Extracting predictions and labels
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # Replacing -100 with the pad token id so that we decode properly
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    # Decode predictions and references
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

    # Normalising predictions and references
    pred_str_norm = [twi_financial_normalizer(p) for p in pred_str]
    label_str_norm = [twi_financial_normalizer(l) for l in label_str]

    # Filtering out empty references to avoid errors in WER computation
    pred_str_norm = [
        pred_str_norm[i] for i in range(len(pred_str_norm))
        if len(label_str_norm[i]) > 0
    ]
    label_str_norm = [
        label_str_norm[i] for i in range(len(label_str_norm))
        if len(label_str_norm[i]) > 0
    ]
    # Ensuring to initialize metrics dictionary
    metrics = {}

    # Computing orthographic WER (no normalization)
    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)
    metrics["wer_ortho"] = wer_ortho # Add metric to dictionary

    # Computing CER
    cer = 100 * compute_character_error_rate(pred_str, label_str)
    metrics["cer"] = cer # Add metric to dictionary

    # Computing WER on normalized text
    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)
    metrics["wer"] = wer # Add metric to dictionary

    return metrics # Return the metrics dictionary

!pip install jiwer
!pip install evaluate

import evaluate

# Loading the Word Error Rate (WER) metric
metric = evaluate.load("wer")

# Defing our custon twi normaliser around here:
import numpy as np
import os
import re
import string
import unicodedata
import torch
import evaluate
from dataclasses import dataclass
from typing import Any, Dict, List, Union
from transformers import WhisperProcessor
from datasets import Audio, Dataset, DatasetDict


# Initialising Processor and Metric

# Assume you have chosen a multilingual model capable of Twi and English code switching.
processor = WhisperProcessor.from_pretrained(
    "openai/whisper-small",
    task="transcribe"  # no language specified, letting Whisper detect language
)

metric = evaluate.load("wer")


# Defining the Twi Financial Normalizer

def twi_financial_normalizer(text: str) -> str:
    text = text.lower()
    text = unicodedata.normalize("NFC", text)

    # Allowed characters include letters, digits, financial symbols, punctuation used in financial contexts
    allowed_chars = set("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ƆƐ")
    text = "".join(ch for ch in text if ch in allowed_chars)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# Step 4: Define the Compute Metrics Function

# def compute_metrics(pred):
#     pred_ids = pred.predictions
#     label_ids = pred.label_ids

#     # Replace -100 with pad_token_id
#     label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

#     # Decode predictions and references
#     pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
#     label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

#     # Compute orthographic WER (no normalization)
#     wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)

#     # Normalize predictions and references
#     pred_str_norm = [twi_financial_normalizer(p) for p in pred_str]
#     label_str_norm = [twi_financial_normalizer(l) for l in label_str]

#     # Filter out empty references to avoid errors in WER computation
#     pred_str_norm = [
#         pred_str_norm[i] for i in range(len(pred_str_norm))
#         if len(label_str_norm[i]) > 0
#     ]
#     label_str_norm = [
#         label_str_norm[i] for i in range(len(label_str_norm))
#         if len(label_str_norm[i]) > 0
#     ]

#     # Compute WER on normalized text
#     wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)

#     return {"wer_ortho": wer_ortho, "wer": wer}


# def compute_metrics(pred):
#     # Extract predictions and labels
#     pred_ids = pred.predictions
#     label_ids = pred.label_ids

#     # Replace -100 with the pad token id so that we decode properly
#     label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

#     # Decode predictions and references
#     pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
#     label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

#     # Normalize predictions and references
#     pred_str_norm = [twi_financial_normalizer(p) for p in pred_str]
#     label_str_norm = [twi_financial_normalizer(l) for l in label_str]

#     # Filter out empty references to avoid errors in WER computation
#     pred_str_norm = [
#         pred_str_norm[i] for i in range(len(pred_str_norm))
#         if len(label_str_norm[i]) > 0
#     ]
#     label_str_norm = [
#         label_str_norm[i] for i in range(len(label_str_norm))
#         if len(label_str_norm[i]) > 0
#     ]

# def compute_character_error_rate(predictions, references):
#     """
#     Compute the Character Error Rate (CER) between a list of predictions and references.
#     CER is defined as the edit distance between the predicted and reference text,
#     divided by the number of characters in the reference, multiplied by 100.
#     """
#     total_distance = 0
#     total_chars = 0
#     for pred, ref in zip(predictions, references):
#         # Remove extra spaces if needed and measure distance
#         pred = pred.strip()
#         ref = ref.strip()

        # Compute edit distance for characters
#         dist = levenshtein_distance(ref, pred)
#         total_distance += dist
#         total_chars += len(ref)

#     # Avoid division by zero if some references are empty
#     if total_chars == 0:
#         return 0.0

#     return total_distance / total_chars


# def levenshtein_distance(s1, s2):
#     """
#     Compute the Levenshtein distance (edit distance) between two strings at the character level.
#     This is a standard dynamic programming approach:
#     - distance[i][j] represents the edit distance between s1[:i] and s2[:j].
#     """
#     m = len(s1)
#     n = len(s2)

#     # Initialize a (m+1)x(n+1) matrix
#     distance = np.zeros((m+1, n+1), dtype=int)

#     # Base cases: distance from empty string
#     for i in range(m+1):
#         distance[i][0] = i
#     for j in range(n+1):
#         distance[0][j] = j

#     # Compute distances
#     for i in range(1, m+1):
#         for j in range(1, n+1):
#             cost = 0 if s1[i-1] == s2[j-1] else 1
#             distance[i][j] = min(
#                 distance[i-1][j] + 1,     # deletion
#                 distance[i][j-1] + 1,     # insertion
#                 distance[i-1][j-1] + cost # substitution
#             )

#     return distance[m][n]

#     # Compute orthographic WER (no normalization)
#     wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)

#     cer = 100 * compute_character_error_rate(pred_str, label_str)

#     # Compute WER on normalized text
#     wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)

#     return {"wer_ortho": wer_ortho, "wer": wer, "cer": cer}
#     # Compute CER (Character Error Rate)
#     # We define a helper function to compute CER for a list of predictions and references

from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

from functools import partial

# disabling cache during training since it's incompatible with gradient checkpointing
model.config.use_cache = False

# setting task for generation and re-enable cache
# removing 'language="sinhalese"' since we're allowing Whisper to detect Twi automatically
model.generate = partial(
    model.generate,
    task="transcribe",
    use_cache=True
)

from functools import partial
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/please",    # directory to save models
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,      # adjust as needed for your GPU resources
    learning_rate=1e-5,
    lr_scheduler_type="constant_with_warmup",
    warmup_steps=500, #change back to 400
    #max_steps=500,                      # increase if you have more training resources/time
    gradient_checkpointing=True,
    fp16=True,
    fp16_full_eval=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=16,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=500, #after every 500 steps the model at that time
    eval_steps=500, #evaluate the model after 500 steps
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    save_total_limit =15,
    num_train_epochs=2
    # push_to_hub=True,                   # Set to False if you do not want to push to Hugging Face Hub
)

# disabling cache during training since it's incompatible with gradient checkpointing
model.config.use_cache = False

# setting task for generation and re-enable cache
# We do not specify language here to allow automatic language detection
model.generate = partial(
    model.generate,
    task="transcribe",
    use_cache=True
)

from transformers import Seq2SeqTrainer
#using the Seq2Seq Trainer since audio is an input sequence and we are outputting a sequence of words
trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice["train"],
    eval_dataset=common_voice["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor
)
#trainer.processing_class = processor.__class__

#training
trainer.train()

# If resuming from checkpoint if it terminates unexpectedly
trainer.train(resume_from_checkpoint=True)